---
title: "Predict salary of programmers in China by supervised learning"
subtitle: 'DSC5103 Final Report'
author: "G2Group09"
date: "Nov 2017"
output:
  html_document:
    highlight: tango
    theme: spacelab
  pdf_document:
    highlight: zenburn
---
<!--
comments must be put in an HTML comment form
-->

```{r set-options, echo=FALSE, cache=FALSE}
options(width = 120)  # set output width
```


## 1. Problem Definition


### a. Problem Statement

Currently, programmer is one of the highest-paid job in China. We are curious about how their salary are distributed, higher salaries are determined by which type of attributes, for instance, personal qualifications like education and skills, company and job attributes like company scale and location.  Also, we try to make a prediction on the salary of different programmers in China. Several supervised learning models are adopted to solve this regression problem. 

Main targets of our project are as follows:

1.  Identifying individual qualifications as well as company attributes’ effect on programmer’s salary in China.

2.  Making robust prediction on individual salary based on machine learning algorithms.

3.  Comparing model performances


-----

### b. Data Collection and Processing

#### Data Collection ####

Our key dataset is from a shared dataset of programmers' recruitment information data which were crawled from www.jobui.com (a Chinese recruitment webseite. 10949 lines of recruitment data were in the raw data set. Search number(hits) of the company name on Baidu were crawled by us to represent company reputation.

Data source: 

```{r}

rawdata <- read.csv("raw_data.csv")
summary(rawdata)
head(rawdata)

```

----

#### Data Processing ####

Salary information on the jobui.com is shown by range. We took the average value of the range as our dependent varaibale 'salary_avg'. Outliers are removed by plotting boxplot.
Column position.name is in text format in Chinese. We want to see how different position affect salary by doing feature engineering. In order to extract key job attributes from position description, we merged similar characters. 

```{r, warning=FALSE}

library("jiebaR")
library("wordcloud")
library("stringr")

# calculate average salary as y and remove outliers
rawdata$salary_avg <- as.numeric((rawdata$salary_lower_bound+rawdata$salary_upper_bound)/2)
box <- boxplot(rawdata$salary_avg)
rawdata <- subset(rawdata,rawdata$salary_avg < 32.5 & rawdata$salary_avg > 6) # remove outliers
data <- rawdata[,-c(1,6,7)]

# text cleaning 
data$position.name <- tolower(as.character(data$position.name)) 
data$position.name <- str_replace_all(data$position.name, "资深", "高级") # combine similar words
data$position.name <- str_replace_all(data$position.name, "web", "前端")
data$position.name <- str_replace_all(data$position.name, "网站", "前端")
data$position.name <- str_replace_all(data$position.name, "客户端", "前端")
data$position.name <- str_replace_all(data$position.name, "服务端", "后端")
data$position.name <- str_replace_all(data$position.name, "服务器", "后端")
data$company.scale <- str_replace_all(data$company.scale, "人", "")
data$company.scale <- str_replace_all(data$company.scale, "以上", "<")
data$company.scale <- str_replace_all(data$company.scale, "以下", "<")
data$company.scale <- str_replace_all(data$company.scale, "少于", ">")
data$company.scale <- as.factor(data$company.scale)
data <- subset(data, str_detect(data$position.name, "实习")==FALSE) # delete internships

```

Subsequently, we did text mining to get the frequency of key attributes of position descriptions. Attributes with word frequency larger than 20 were picked. Some common words which cannot differentiate programmers' positon were not picked, such as "engineer" and "software".

```{r, warning=FALSE}

# use package "fenci" to get Chinese words in strings
wk = worker()
position <- wk[data$position.name]
position_freq <- table(position) # word frequency table
seg <- sort(position_freq, decreasing = TRUE)[1:100]

# pick up words we want: 
words <- c("高级","中级","初级","前端","后端","开发","架构师","运维","游戏","数据","算法","研发")
position.key <- position[position %in% words]
position.key.freq <- table(position.key) # word frequency table
seg.key <- sort(log(position.key.freq), decreasing = TRUE) # take log so as to make the graph pretty

# wordcloud
colors=brewer.pal(6,"Dark2")
set.seed(123)
words.en <- c("senior", "intermediate",	"junior",	"front.end", "back.end", "develop", "architect", "operation",	"game",	"data", "algorithm", "R.D")
wordcloud(words.en, seg.key,colors=colors,random.order=F)

```


```{r}

# add dummies to data
for(j in 1:length(words)){
  for (i in 1:nrow(data)){
    data[i, j+10] <- ifelse(words[j] %in% wk[data$position.name[i]], 1, 0)
  }
  names(data)[j+10] <- words.en[j]
}

```

Conduct numeric variable of work experience:

```{r}
# change work experience
data$work.experience.lowerb <- ifelse(data$work.experience=="1-3year",1,ifelse(data$work.experience=="3-5year", 3, ifelse(data$work.experience=="5-10year", 5, ifelse(data$work.experience=="less than 1 year"|data$work.experience=="freshgraduate"|data$work.experience=="unlimited", 0, 10))))
data$work.experience.upperb <- ifelse(data$work.experience=="1-3year",3,ifelse(data$work.experience=="3-5year", 5, ifelse(data$work.experience=="5-10year", 10, ifelse(data$work.experience=="less than 1 year"|data$work.experience=="unlimited", 1, ifelse(data$work.experience=="freshgraduate", 0, 20)))))

#delete useless columns
data <- data[,-c(2,8)]
head(data)

```

The cleaned dataset contains 22 columns:

 - **required.language**: Categorical variable with 5 classes-C#, C++, Java, PHP and Python. This field gives the set of expected software programming language skillset of the job openings.
 - **work.experience**: Categorical variable with 7 classes - 1-3 year, 3-5 year, 5-10 year, freshgraduate, less than 1 year, over 10 years, unlimited.
 - **education**: Educational qualification of each person. categorical variable with 4 classes - Bachelor, Junior college, master and unlimited.
 - **company.scale**: Number of employees already working in the company. Categorical with 6 classes - 15-50(low), 50-150(small), 150-500(medium), 500-2000(big), >2000(very big) companies.
 - **location**: City name of the company.
 - **series.level**: Current financing stage of the company. Categorical variable with 7 classes - startup, listed company, others, etc.
 - **search.num**: Total number of search hits of each company.
 - **salary_avg**: The average of salary lower bound and higher bound.
 - "senior",	"intermediate",	"junior",	"front.end",	"back.end",	"develop",	"architect",	"operation",	"game",	"data",	"algorithm",	"R.D": Whether the position.name contains these key words. "1" denotes yes and "0" denotes no.
 - **work.experience.lowerb**: Lower bound of work experience required.
 - **work.experience.higherb**: Upper bound of work experience required.


-----

### c. Descriptive Statistics/ Data Analysis

-----

#### Data Analysis ####

salary distribution in the whole group:

```{r}
library("ggplot2")
# histogram of salary
ggplot(data, aes(x = salary_avg)) +geom_histogram(binwidth = 2.5, fill = "darkorange", colour = "black") +scale_x_continuous(breaks=seq(0, 30, 5))+geom_vline(aes(xintercept=mean(salary_avg)), color="red", linetype="dashed", size=1)

```

----


## 2. Analysis Execution


### Methodology

Dependent variable = Salary_avg
Predictors = Individual attributes + Company attributes

Build the optimum models using stepAIC, lasso, random forest, xgboost, svm and ensemble for the best test accuracy. These models are chosen for their ability on predicting high dimensional predictors.

Data are spilt to three sets: train data, blender data and test data. We take train data to train single model and predict with blender data and test data. After building single models, we train an ensemble model by combining single models and use blender data as the train data for it. In the end, different models are compared by test data **RMSE**.

----

### Model Building

spilt full data into 3 sets: train set, blender set and test set:

```{r}

N <- nrow(data)
set.seed(123)
train.index <- sample(1:N, round(N/2))
datahalf <- data[-train.index,]
M <- nrow(datahalf)
blender.index <- sample(1:M, round(M/2))
test.index <- -blender.index

x.train.m <- model.matrix(salary_avg ~ ., data[train.index, ])[, -1]
y.train <- data[train.index, "salary_avg"]
x.blender.m <- model.matrix(salary_avg ~ ., datahalf[blender.index, ])[, -1]
y.blender <- datahalf[blender.index, "salary_avg"]
x.test.m <- model.matrix(salary_avg ~ ., datahalf[test.index, ])[, -1]
y.test <- datahalf[test.index, "salary_avg"]

x.train <- data[train.index, -8]
x.blender <- datahalf[blender.index, -8]
x.test <- datahalf[test.index, -8]

```



----

#### Linear model (without regularization)

Since number of predictors is quite high subset selection (2^p) is computationally infeasible. So stepwise is chosen. Since we have a very large dataset, we are using backward as forward works well when p>>n.

```{r}

library("MASS")
lm.all <- lm(salary_avg ~ .-work.experience.lowerb-work.experience.upperb , data = data, subset = train.index)
lm.mod <- stepAIC(lm.all, direction="backward")
summary(lm.mod)

```

Variables eliminated from stepAIC are "game", "front-end", "operation", "develop", which means that these variables are not important in predicting salary.


```{r}

# prediction on test data
lm.pred <- predict(lm.mod, datahalf[test.index,-c(8,21,22)])
rmse.lm <- sqrt(mean((lm.pred-y.test)^2))
lm.blender <- predict(lm.mod, x.blender)

```


----

#### Regularized linear model

```{r, warning=FALSE}

# construct model matrix for lasso
library("glmnet")
set.seed(123)
fold <- sample(rep(seq(10), length=nrow(x.train)))

# lasso
lasso.cv <- cv.glmnet(x.train.m, y.train, alpha=1, foldid = fold)
lasso.lam <- lasso.cv$lambda.1se

predict(lasso.cv, type="coefficient", s=lasso.lam, exact=TRUE)
# predicitions
lasso.pred <- predict(lasso.cv, newx=x.test.m, s=lasso.lam)
rmse.lasso <- sqrt(mean((lasso.pred - y.test)^2))
lasso.blender <- predict(lasso.cv, newx=x.blender.m, s=lasso.lam)

```

From the results, Lasso eliminates predictors front-end, develop and education master, work.ex (3-5) and (5-10), company scale(<15), educationmaster, series.leveldeveloping (no financing need) as not important.


```{r, warning=FALSE}
# Elastic Net
# candidates for alpha
alphas <- seq(0, 1, 0.05)
# cross-validation to find the best alpha-lambda combination
en.cv.error <- data.frame(alpha=alphas)
for (i in 1:length(alphas)){
  en.cv <- cv.glmnet(x.train.m, y.train, alpha=alphas[i], foldid=fold)
  en.cv.error[i, "lambda.1se"] <- en.cv$lambda.1se
  en.cv.error[i, "error.1se"] <- min(en.cv$cvm) + en.cv$cvsd[which.min(en.cv$cvm)]
}

en.lam <- en.cv.error[which.min(en.cv.error$error.1se), "lambda.1se"]
en.alpha <- en.cv.error[which.min(en.cv.error$error.1se), "alpha"]

en.mod <- glmnet(x.train.m, y.train, alpha=en.alpha)
predict(en.mod, type="coefficient", s=en.lam, exact=TRUE)

# prediction
en.pred <- predict(en.mod, newx=x.test.m, s=en.lam)
rmse.en <- sqrt(mean((en.pred - y.test)^2))
en.blender <- predict(en.mod, newx=x.blender.m, s=en.lam)

```

Optimal alpha of Elastic Net is 0.05. EN is giving very close results compared to Lasso as the optimium is 95% ridge Except it included all eliminated predictors back except company scale <15. Elastic net model is chosen to see any performance gain in test error. As we can see, there is improvement in test accuracy but the inferences are still the same.

----

#### Random Forest

```{r, eval=FALSE, echo=TRUE, warning=FALSE}

library("randomForest")
mse.rfs <- c()
for(m in 1:21){
  set.seed(12)
  rf <- randomForest(salary_avg ~ ., data=data, subset=train.index, mtry=m)
  mse.rfs[m] <- rf$mse[500]
}
opt.num = which.min(mse.rfs)

```

```{r}
library("randomForest")
opt.num <- 7
set.seed(123)
rf <- randomForest(salary_avg ~ ., data=data, subset = train.index, mtry = opt.num)
rf.pred <- predict(rf,newdata = x.test)
rmse.rf <- sqrt(mean((rf.pred-y.test)^2))
rf.blender <- predict(rf,newdata = x.blender)
```




```{r}

varImpPlot(rf)
partialPlot(rf, x.train, x.var="location")
partialPlot(rf, x.train, x.var="search.num")
partialPlot(rf, x.train, x.var="required.language")
partialPlot(rf, x.train, x.var="work.experience.lowerb")

```

The variable importance plot shows that location is the most important predictors in predicting programmer's salary, because salaries differ a lot between first-tier cities and other cities. Specifically, programmers in Beijing have the highest salary while Shanghai is the 2nd highest on average. Other two cities with relatively high income are Shenzhen and Hangzhou. These four cities have high average income for programmers because of well-performed district economies and having Chinese internet leaders like Tencent, Alibaba, Baidu etc set their headquarters in these cities.

The second most important predictors in our dataset is search.num, which is the number of information that we can search on Baidu about the company. This variable can be a measure of the company's social reputation to some extent,which means that the reputation of a company will have great influence on a programmer's salary. Based on the partial dependence plot, company with higher reputation will pay higher for their programmers, which corresponds to common sense.

From personal perspective, important predictors include work experience and education, as in technical position, experience and knowledge is a very important factor influencing income.

One interesting finding is that within all the programming languages, python seems to be the most promising programming language. Programmers familiar with python can get slightly higher pays than other language users.


----

#### GBM

Tuning code:

```{r, eval=FALSE, echo=TRUE, warning=FALSE}

library("xgboost")
dtrain <- xgb.DMatrix(data=x.train.m, label=y.train)
objective <- "reg:linear"
cv.fold <- 10

# parameter ranges
max_depths <- c(1, 2, 4, 6, 7, 8)
etas <- c(0.01, 0.005, 0.001)
subsamples <- c(0.5, 0.75, 1)
colsamples <- c(0.6, 0.8, 1)

set.seed(123)
tune.out <- data.frame()
for (max_depth in max_depths) {
  for (eta in etas) {
    for (subsample in subsamples) {
      for (colsample in colsamples) {
        n.max <- round(100 / (eta * sqrt(max_depth)))
        xgb.cv.fit <- xgb.cv(data = dtrain, objective=objective, nfold=cv.fold, early_stopping_rounds=100, verbose=0,
                             nrounds=n.max, max_depth=max_depth, eta=eta, subsample=subsample, colsample_bytree=colsample)
        n.best <- xgb.cv.fit$best_ntreelimit
        if (objective == "reg:linear") {
          cv.err <- xgb.cv.fit$evaluation_log$test_rmse_mean[n.best]
        } else if (objective == "binary:logistic") {
          cv.err <- xgb.cv.fit$evaluation_log$test_error_mean[n.best]
        }
        out <- data.frame(max_depth=max_depth, eta=eta, subsample=subsample, colsample=colsample, n.max=n.max, nrounds=n.best, cv.err=cv.err)
        tune.out <- rbind(tune.out, out)
      }
    }
  }
}
opt <- which.min(tune.out$cv.err)
max_depth.opt <- tune.out$max_depth[opt]
eta.opt <- tune.out$eta[opt] 
subsample.opt <- tune.out$subsample[opt] 
colsample.opt <- tune.out$colsample[opt] 
nrounds.opt <- tune.out$nrounds[opt] 

```

After tuning, we get the optimal result:

```{r, warning=FALSE}

library("xgboost")
dtrain <- xgb.DMatrix(data=x.train.m, label=y.train)
dblender <- xgb.DMatrix(data=x.blender.m, label=y.blender)
dtest <- xgb.DMatrix(data=x.test.m, label=y.test)

# tuning result
max_depth.opt <- 7
eta.opt <- 0.05
subsample.opt <- 1
colsample.opt <- 1

set.seed(123)
xgb.cv <- xgb.cv(data = dtrain,  objective="reg:linear", nrounds=5000, max_depth=max_depth.opt, eta=eta.opt, subsample=subsample.opt, colsample_bytree=colsample.opt, nfold=10, early_stopping_rounds=100, verbose = 0)
nrounds.opt <- xgb.cv$best_ntreelimit

# train model
xgb.mod <- xgboost(data=dtrain, objective="reg:linear", nround=nrounds.opt, max.depth=max_depth.opt, eta=eta.opt, subsample=subsample.opt, colsample_bytree=colsample.opt, verbose = 0)
xgb.pred <- predict(xgb.mod, newdata=dtest)
rmse.xgb <- sqrt(mean((xgb.pred-y.test)^2))
xgb.blender <- predict(xgb.mod, newdata=dblender)

```


```{r, warning=FALSE}
importance_matrix <- xgb.importance(model = xgb.mod, feature_names = colnames(x.train.m))
xgb.plot.importance(importance_matrix=importance_matrix)

library("pdp")
plotPartial(partial(xgb.mod, train=x.train.m, pred.var = "work.experience.lowerb", chull = TRUE))
plotPartial(partial(xgb.mod, train=x.train.m, pred.var = "search.num", chull = TRUE))

```

What we find interesting here is that though the GBM performs almost as well as random forest model, the importance plot and partial dependence plots of GBM are different from that of random forest model. In GBM, the most important variable is work.experience.lowerb and the partial relationship between search.num and salary is not monotone.

GBM shows that for programmers with higher work experience, they are more likely to have a higher salary and the increase of salary is significant during the first 5 years' career; for programmers with 5 or more years of work experience, their salary won't increase due to longer work experience, which is the same as random forest.

But for search.num, random forest and GBM gives different results: in random forest, the higher the company awareness is, the higher the salary they pay for programmers; but in GBM, it seems that companies with search.num from 10000000 to 30000000 pay higher salary than those who have a higher or lower search.num.


----

#### SVM

Tuning code:
```{r, echo = TRUE, eval = FALSE}

library("e1071")
costs <- c(0.00001,0.0001,0.001,0.01,0.1,1,10)
gammas <- c(0.01,0.02,0.03,0.04,0.05,0.10,0.2,0.3,0.4,0.5)
kernels <- c('linear','polynomial','radial','sigmoid')
svm.tune.result <- expand.grid(cost = costs, gamma = gammas, kernel = kernels)
svm.tune.result['MSE'] <- 0
for (i in costs){
  for (j in gammas){
    for (x in kernels){
      svmfit <- svm(salary_avg ~ ., data=data[train.index,],gamma = j, cost = i, kernel = x, cross = 5)
      svm.tune.result[svm.tune.result$cost == i & svm.tune.result$gamma == j & svm.tune.result$kernel == x,4] <- svmfit$tot.MSE[1]
      print(svmfit$tot.MSE[1])
    }
  }
}

opt.index <- which.min(svm.tune.result$MSE)
opt.cost <- svm.tune.result$cost[opt.index]
opt.gamma <- svm.tune.result$gamma[opt.index]
opt.kernel <- svm.tune.result$kernel[opt.index]

```


After tuning, we get the optimal result:

```{r, warning=FALSE}

library("e1071")
opt.cost <- 10
opt.gamma <- 0.01
opt.kernel <- "radial"
opt.svm <- svm(salary_avg ~ ., data=data[train.index,], gamma = opt.gamma, cost = opt.cost, kernel = opt.kernel)
svm.pred <- predict(opt.svm, newdata = x.test)
rmse.svm <- sqrt(mean((svm.pred - y.test)^2))
svm.blender <- predict(opt.svm, newdata = x.blender)

```


----

#### Ensemble model

Single models are combined together to conduct both a lassso model and a randomForest model. 
```{r}
# combine data
train.data.ensemble <- data.frame(lm.blender,lasso.blender,en.blender,rf.blender,xgb.blender,svm.blender,y.blender)
colnames(train.data.ensemble) <- c("lm","lasso","en","rf","xgb","svm","y")

test.data.ensemble <- data.frame(lm.pred,lasso.pred,en.pred,rf.pred,xgb.pred,svm.pred,y.test)
colnames(test.data.ensemble) <- c("lm","lasso","en","rf","xgb","svm","y")

# lm
x.ensemble.m <- model.matrix(y~.,train.data.ensemble)[,-1]
y.ensemble <- train.data.ensemble$y
x.ensemble.test.m <- model.matrix(y~.,test.data.ensemble)[,-1]
y.ensemble.test <- test.data.ensemble$y

lm.ensemble.cv <- cv.glmnet(x.ensemble.m, y.ensemble, alpha=1)

ensemble.lam <- lm.ensemble.cv$lambda.1se
ensemble.lasso.mod <- glmnet(x.ensemble.m, y.ensemble, alpha=1)

# predicitions
ensemble.lasso.pred <- predict(ensemble.lasso.mod, newx=x.ensemble.test.m, s=ensemble.lam)
rmse.ensemble.lm <- sqrt(mean((ensemble.lasso.pred - y.ensemble.test)^2))

```

Since random forest performs better than linear model here, we choose it as the final ensemble model.

```{r}
# random forest
set.seed(123)
rf.ensemble <- randomForest(y ~ ., data=train.data.ensemble)
rf.ensemble.pred <- predict(rf.ensemble,newdata = test.data.ensemble[,-7])
rmse.rf.ensemble <- sqrt(mean((rf.ensemble.pred-test.data.ensemble[,7])^2))

```


----

### Model Comparison

```{r}
library(ggplot2)
model <- c('en','lasso','lm','rf','svm','xgb','ensemble.lm','ensemble.rf')
rmse <- c(rmse.en,rmse.lasso,rmse.lm,rmse.rf,rmse.svm,rmse.xgb,rmse.ensemble.lm,rmse.rf.ensemble)
comparison <- data.frame(model,rmse)
comparison <- comparison[order(comparison$rmse),]
ggplot(data = comparison) + geom_col(aes(reorder(model, rmse),rmse,fill=rmse))

par(mfrow=c(3,3))
plot(rf.ensemble.pred,y.test)
plot(rf.pred,y.test)
plot(xgb.pred,y.test)
plot(ensemble.lasso.pred,y.test)
plot(lm.pred,y.test)
plot(svm.pred,y.test)
plot(en.pred,y.test)
plot(lasso.pred,y.test)

```


On an overall aspect, ensemble model using random forest performs the best. Tree based methods like random forest and xgboost follow, while linear models perform a little bit worse. In addition, some problems that can be seen from the prediction plot are as follows: 

(1) linear regressions like EN, lm, lasso don't perform well in predicting high salaries. The predictions of high salaries in linear model tend be lower than real value. 

(2) Tree based methods like random forest and xgboost perform better in high salary part, but have higher variance in the middle level salary predictions. This is because tree-based methods take average of leaf nodes to be predictions while linear regression will be relatively fixed by middle level majority data, thus not able to predict extreme values well.

----


## 3. Conclusion

 - Ensemble random forest model performs the best within all the models, whose RMSE is 3.68. This means that on average the prediction and the real salary differ around 3.68 (k).
 - On the company side, location, reputation and series level of the company affect a programmer's salary most.
 - On individual side, work experience, education and programming languages have effect on programmer's salary.
 - If you are looking forward to high income and planning to work as a programmer, learn Python and try to find companies in series C or D, this gives the highest probability for your to be paid high.

----



## 4. Future Scope

 - Our data has some limitation, because most of our variables are categorical variables, which might not be strong enough to provide enough information for the model regression. We might need to search for more numeric data to predict a better result.
 - The salary data we are using is only a claimed salary released on job hunting websites, which is different from the real salary data, we can try to find more accurate salary data to do the prediction.
 - In feature engineering, we split the position name information into many different dummies. However, some of the names are not clean and word splitting done by JiebaR has some mistakes, which might lead to information loss and dummy imbalance, we can try to find better ways to extract information from position names.

----



***[THE END]***